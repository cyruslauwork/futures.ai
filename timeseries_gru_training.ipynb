{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FARZM7X0-y_b",
        "outputId": "20f749b1-ed74-4641-bd30-85371ce0adea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hktEy2stK4Vo",
        "outputId": "a06d8a5e-b3f1-496c-c423-2318cf243665"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pandas version 2.2.2 is already installed.\n",
            "numpy version 1.26.4 is already installed.\n",
            "scikit-learn not installed. Installing scikit-learn version 1.6.0...\n",
            "Requirement already satisfied: scikit-learn==1.6.0 in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.0) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.0) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.0) (3.5.0)\n",
            "torch version 2.5.1 is already installed.\n",
            "joblib version 1.4.2 is already installed.\n",
            "matplotlib version 3.10.0 is already installed.\n",
            "pandas version: 2.2.2\n",
            "numpy version: 1.26.4\n",
            "scikit-learn version: 1.6.0\n",
            "torch version: 2.5.1+cu124\n",
            "joblib version: 1.4.2\n",
            "matplotlib version: 3.10.0\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "# Function to check and install specific package versions\n",
        "def install_package(package, version):\n",
        "    try:\n",
        "        # Check the current version\n",
        "        current_version = __import__(package).__version__\n",
        "\n",
        "        # Special handling for torch to check for version with CUDA suffix\n",
        "        if package == \"torch\":\n",
        "            if current_version.startswith(version):\n",
        "                print(f\"{package} version {version} is already installed.\")\n",
        "                return\n",
        "        else:\n",
        "            if current_version == version:\n",
        "                print(f\"{package} version {version} is already installed.\")\n",
        "                return\n",
        "\n",
        "        print(f\"Uninstalling {package} version {current_version}...\")\n",
        "        !pip uninstall -y {package}\n",
        "        print(f\"Installing {package} version {version}...\")\n",
        "        !pip install {package}=={version}\n",
        "\n",
        "    except ImportError:\n",
        "        print(f\"{package} not installed. Installing {package} version {version}...\")\n",
        "        !pip install {package}=={version}\n",
        "\n",
        "# Specify the packages and their desired versions\n",
        "packages = {\n",
        "    \"pandas\": \"2.2.2\",\n",
        "    \"numpy\": \"1.26.4\",\n",
        "    \"scikit-learn\": \"1.6.0\",\n",
        "    \"torch\": \"2.5.1\",\n",
        "    \"joblib\": \"1.4.2\",\n",
        "    \"matplotlib\": \"3.10.0\"\n",
        "}\n",
        "\n",
        "# Check and install packages\n",
        "for pkg, ver in packages.items():\n",
        "    install_package(pkg, ver)\n",
        "\n",
        "# Import the necessary libraries after ensuring correct versions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "import joblib\n",
        "import matplotlib as mpl  # Import matplotlib for version checking\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, time\n",
        "import random\n",
        "from sklearn import __version__ as sklearn_version  # Import sklearn version\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "# Print package versions\n",
        "print(\"pandas version:\", pd.__version__)\n",
        "print(\"numpy version:\", np.__version__)\n",
        "print(\"scikit-learn version:\", sklearn_version)\n",
        "print(\"torch version:\", torch.__version__)\n",
        "print(\"joblib version:\", joblib.__version__)\n",
        "print(\"matplotlib version:\", mpl.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SJl7Rb-SK8Eq"
      },
      "outputs": [],
      "source": [
        "# DETREND_WINDOW_SIZE = 2\n",
        "BATCH_SIZE = 2048\n",
        "WINDOW_SIZE = 50\n",
        "LEARNING_RATE = 0.001  # From 0.001 to the best 0.0001\n",
        "EPOCH_SIZE = 3 # 50\n",
        "PREDICTION_LEN = 1   # Number of time steps to predict each time\n",
        "K_FOLDS = 3 # 10  # Number of folds for cross-validation\n",
        "TIME_INTERVAL_SEC = 60  # For 1 minute interval: 60\n",
        "PREDICTION_TIMESTEP_SPAN = 0  # 14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mylLY_yiLUwm",
        "outputId": "f1c80f9b-7002-44f8-93c0-9a4254c145d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Colab Notebooks/futures.ai/gru_close_dynamic_scaler.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Load the JSON data\n",
        "# data = pd.read_json('/content/drive/MyDrive/Colab Notebooks/futures.ai/spy_1min_regularhours_truncated_preprocessed.json')\n",
        "data = pd.read_json('/content/drive/MyDrive/Colab Notebooks/futures.ai/spy_1min_regularhours_truncated_semisupervised_preprocessed.json')\n",
        "\n",
        "# Convert 'datetime' strings back to datetime objects\n",
        "data['datetime'] = pd.to_datetime(data['datetime'])\n",
        "\n",
        "## De-trend the data\n",
        "#\n",
        "# De-trending helps to remove any long-term trends or seasonality from the data,\n",
        "# allowing the model to focus on the fluctuations that matter most for prediction.\n",
        "#\n",
        "# Why De-trend Before Normalization?\n",
        "# Focus on Fluctuations:\n",
        "# Normalizing data with trends can make it difficult for the model to learn meaningful patterns,\n",
        "# as the model may focus on the trends rather than the actual relationships between features.\n",
        "# Consistent Scale:\n",
        "# By de-trending all relevant features, you ensure that they are on a similar scale, which can improve the learning process.\n",
        "FEATURES_TO_DETREND = ['close', 'EMA_5', 'EMA_10', 'EMA_15', 'EMA_20']  #'open', 'high', 'low',\n",
        "FEATURES_NOT_TO_DETREND = ['LONG_UPPER_SHADOW', 'LONG_LOWER_SHADOW', 'EMA_5_EMA_10', 'EMA_15_EMA_20', 'RSI', 'RSI_INT']\n",
        "# FEATURES_NOT_TO_DETREND = ['RSI']\n",
        "ALL_FEATURES = FEATURES_TO_DETREND + FEATURES_NOT_TO_DETREND\n",
        "\n",
        "for feature in FEATURES_TO_DETREND:\n",
        "    # SMA for de-trending\n",
        "    #\n",
        "    # Choosing the Window Size for Rolling Mean\n",
        "    #\n",
        "    # If your data has short-term fluctuations, a smaller DETREND_WINDOW_SIZE (e.g., 5-10)\n",
        "    # might be more appropriate, allowing you to capture more immediate trends.\n",
        "    # For longer-term trends, a larger DETREND_WINDOW_SIZE (e.g., 30-60) is often used\n",
        "    # to smooth out more significant fluctuations and focus on overarching trends.\n",
        "    #\n",
        "    # Consider what you are trying to predict. If you are interested in short-term price movements,\n",
        "    # a smaller window might be better. For long-term predictions, a larger window can help smooth out noise.\n",
        "    #\n",
        "    # If your goal is to analyze short-term trends without losing the fluctuations, you might consider using a very short window size (e.g., 2 or 3)\n",
        "    # for a moving average. This would still smooth the data slightly but would retain more of the original fluctuations compared to larger windows.\n",
        "    # data[f'{feature}_trend'] = data[feature].rolling(window=DETREND_WINDOW_SIZE).mean()  # Simple moving average\n",
        "    # data[f'{feature}_detrended'] = data[feature] - data[f'{feature}_trend']\n",
        "\n",
        "    # Differencing for de-trending\n",
        "    data[f'{feature}_detrended'] = data[feature].diff()  # Calculate the difference from the previous value\n",
        "data = data.dropna()    # Drop the first row which will be NaN due to differencing or drop NaN values created by rolling mean\n",
        "\n",
        "## Normalize the de-trended features\n",
        "\n",
        "# Define fixed scaler for RSI\n",
        "class FixedRangeScaler:\n",
        "    def __init__(self, feature_range=(0, 1), input_range=(0, 100)):\n",
        "        self.feature_range = feature_range\n",
        "        self.input_range = input_range\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_std = (X - self.input_range[0]) / (self.input_range[1] - self.input_range[0])\n",
        "        X_scaled = X_std * (self.feature_range[1] - self.feature_range[0]) + self.feature_range[0]\n",
        "        return X_scaled\n",
        "\n",
        "    def inverse_transform(self, X):\n",
        "        X_std = (X - self.feature_range[0]) / (self.feature_range[1] - self.feature_range[0])\n",
        "        X_original = X_std * (self.input_range[1] - self.input_range[0]) + self.input_range[0]\n",
        "        return X_original\n",
        "\n",
        "# Initialize scalers\n",
        "dynamic_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "long_upper_shadow_dynamic_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "long_lower_shadow_dynamic_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "rsi_scaler = FixedRangeScaler(feature_range=(0, 1), input_range=(0, 100))\n",
        "rsi_int_scaler = FixedRangeScaler(feature_range=(0, 1), input_range=(-1, 1))\n",
        "\n",
        "data = data.copy()  # Create a copy to avoid SettingWithCopyWarning\n",
        "data[[f'{feature}_normalized' for feature in FEATURES_TO_DETREND]] = dynamic_scaler.fit_transform(\n",
        "    data[[f'{feature}_detrended' for feature in FEATURES_TO_DETREND]].values\n",
        ")\n",
        "data['LONG_UPPER_SHADOW_normalized'] = long_upper_shadow_dynamic_scaler.fit_transform(data['LONG_UPPER_SHADOW'].values.reshape(-1, 1))\n",
        "data['LONG_LOWER_SHADOW_normalized'] = long_lower_shadow_dynamic_scaler.fit_transform(data['LONG_LOWER_SHADOW'].values.reshape(-1, 1))\n",
        "data['EMA_5_EMA_10_normalized'] = data['EMA_5_EMA_10']\n",
        "data['EMA_15_EMA_20_normalized'] = data['EMA_15_EMA_20']\n",
        "data['RSI_normalized'] = rsi_scaler.transform(data['RSI'].values.reshape(-1, 1))\n",
        "data['RSI_INT_normalized'] = rsi_int_scaler.transform(data['RSI_INT'].values.reshape(-1, 1))\n",
        "\n",
        "# Save the normalized and de-trended data for future use\n",
        "# data.to_json('processed_data.json', orient='records')\n",
        "\n",
        "# Save the scalers\n",
        "joblib.dump(dynamic_scaler, '/content/drive/MyDrive/Colab Notebooks/futures.ai/gru_dynamic_scaler.pkl')\n",
        "joblib.dump(long_upper_shadow_dynamic_scaler, '/content/drive/MyDrive/Colab Notebooks/futures.ai/gru_long_upper_shadow_dynamic_scaler.pkl')\n",
        "joblib.dump(long_lower_shadow_dynamic_scaler, '/content/drive/MyDrive/Colab Notebooks/futures.ai/gru_long_lower_shadow_dynamic_scaler.pkl')\n",
        "joblib.dump(rsi_scaler, '/content/drive/MyDrive/Colab Notebooks/futures.ai/gru_rsi_scaler.pkl')\n",
        "joblib.dump(rsi_int_scaler, '/content/drive/MyDrive/Colab Notebooks/futures.ai/gru_rsi_int_scaler.pkl')\n",
        "\n",
        "# Fit a dedicated scaler just for 'close'\n",
        "close_dynamic_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "data['close_normalized'] = close_dynamic_scaler.fit_transform(data[['close_detrended']].values)\n",
        "\n",
        "# Save the close scaler\n",
        "joblib.dump(close_dynamic_scaler, '/content/drive/MyDrive/Colab Notebooks/futures.ai/gru_close_dynamic_scaler.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-yZX991GL_Mx"
      },
      "outputs": [],
      "source": [
        "## Define a PyTorch Dataset\n",
        "# Modified TimeSeriesDataset with strict day boundary checks for day-after-day training\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.sequence_length = WINDOW_SIZE\n",
        "        self.prediction_length = PREDICTION_LEN\n",
        "\n",
        "        # Convert the 'datetime' strings to numpy datetime64 for efficient processing\n",
        "        self.dates = np.array([np.datetime64(dt) for dt in data['datetime']])\n",
        "\n",
        "        # Extract features as numpy arrays\n",
        "        self.features = np.stack([\n",
        "            data[f'{feature}_normalized'].values\n",
        "            for feature in ALL_FEATURES\n",
        "        ], axis=1)  # Shape: (n_samples, n_features)\n",
        "\n",
        "        # Store indices for each day\n",
        "        unique_dates = np.unique(self.dates.astype('datetime64[D]'))\n",
        "        print(f'unique_dates: {unique_dates}')\n",
        "        print(f'unique_dates length: {len(unique_dates)}')\n",
        "        self.daily_indices = {\n",
        "            date: np.where(self.dates.astype('datetime64[D]') == date)[0] for date in unique_dates\n",
        "        }\n",
        "\n",
        "        # Pre-compute valid indices for each day\n",
        "        self.valid_indices = []\n",
        "        for date, day_indices in self.daily_indices.items():\n",
        "            if len(day_indices) == 0:\n",
        "                continue  # Skip if there are no indices for this date\n",
        "\n",
        "            for i in range(len(day_indices)):\n",
        "                seq_start = i\n",
        "                seq_end = i + self.sequence_length\n",
        "                target_start = seq_end + PREDICTION_TIMESTEP_SPAN\n",
        "                target_end = target_start + self.prediction_length\n",
        "\n",
        "                # Check if all indices are within the same day\n",
        "                if target_end <= len(day_indices):\n",
        "                    # Verify time continuity\n",
        "                    if self._verify_time_continuity(day_indices, seq_start, seq_end, target_start, target_end):\n",
        "                        self.valid_indices.append((\n",
        "                            day_indices[seq_start:seq_end],\n",
        "                            day_indices[target_start:target_end]\n",
        "                        ))\n",
        "        print(f'self.valid_indices head: {self.valid_indices[:10]}')\n",
        "        print(f'self.valid_indices tail: {self.valid_indices[-10:]}')\n",
        "        print(f'self.valid_indices length: {len(self.valid_indices)}')\n",
        "\n",
        "    def _verify_time_continuity(self, day_indices, seq_start, seq_end, target_start, target_end):\n",
        "        # Verify that times are continuous\n",
        "        for j in range(seq_start, seq_end - 1):\n",
        "            t1 = self.dates[day_indices[j]]\n",
        "            t2 = self.dates[day_indices[j + 1]]\n",
        "            if (t2 - t1).astype('timedelta64[s]') != TIME_INTERVAL_SEC:\n",
        "                return False\n",
        "\n",
        "        for j in range(target_start, target_end - 1):\n",
        "            t1 = self.dates[day_indices[j]]\n",
        "            t2 = self.dates[day_indices[j + 1]]\n",
        "            if (t2 - t1).astype('timedelta64[s]') != TIME_INTERVAL_SEC:\n",
        "                return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def __len__(self):\n",
        "        # Total number of items across all days\n",
        "        return len(self.valid_indices)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        seq_indices, target_indices = self.valid_indices[index]\n",
        "\n",
        "        # Return data in (features, sequence) format\n",
        "\n",
        "        # Get input sequence\n",
        "        x = self.features[seq_indices]  # Shape: (sequence_length, n_features)\n",
        "        x = torch.tensor(x, dtype=torch.float32)\n",
        "\n",
        "        # Get target sequence\n",
        "        # Change in target extraction\n",
        "        # y = self.features[target_indices]   # Target is now all normalized features for the next timestep(s)\n",
        "        # y = torch.tensor(y, dtype=torch.float32)  # Shape: (prediction_length, n_features)\n",
        "        y = self.features[target_indices][:, 0]  # Only extracting the 'close' price\n",
        "        y = torch.tensor(y, dtype=torch.float32).view(-1, 1)  # Shape: (prediction_length, 1)\n",
        "\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "w0Tpxg4bWv3m"
      },
      "outputs": [],
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GRU, self).__init__()\n",
        "        self.input_size = len(ALL_FEATURES)\n",
        "        # Why self.input_size = WINDOW_SIZE * len(ALL_FEATURES) and not self.input_size = len(ALL_FEATURES)?\n",
        "        # In the ANN model, the input is flattened into a single vector because ANNs (fully connected networks)\n",
        "        # do not inherently handle sequential data. The input shape is transformed from (batch_size, sequence_length, n_features)\n",
        "        # to (batch_size, sequence_length * n_features) using nn.Flatten(). This means the entire sequence is treated as a single input vector.\n",
        "        # However, in the GRU model, the input is processed sequentially,\n",
        "        # and the GRU layer expects the input to have the shape (batch_size, sequence_length, n_features). Here:\n",
        "        # - sequence_length is the number of time steps in the sequence (e.g., WINDOW_SIZE).\n",
        "        # - n_features is the number of features at each time step (e.g., len(ALL_FEATURES)).\n",
        "        # So, the GRU model does not flatten the input. Instead, it processes the sequence step by step,\n",
        "        # maintaining the temporal structure of the data. Therefore:\n",
        "        # - input_size in the GRU model refers to the number of features at each time step (len(ALL_FEATURES)),\n",
        "        # not the total size of the flattened input (WINDOW_SIZE * len(ALL_FEATURES)).\n",
        "        self.hidden_size = 256\n",
        "        self.num_layers = 2\n",
        "        self.output_size = PREDICTION_LEN  # Output Layer with only the 'close' price\n",
        "\n",
        "        # GRU Layer\n",
        "        self.gru = nn.GRU(\n",
        "            self.input_size,\n",
        "            self.hidden_size,\n",
        "            self.num_layers,\n",
        "            batch_first=True,\n",
        "            # dropout=0.2 if self.num_layers > 1 else 0\n",
        "            )\n",
        "\n",
        "        # Fully connected layer (Dense) to map GRU output to the desired output size\n",
        "        # self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "        # Additional Dense layers\n",
        "        self.fc1 = nn.Linear(self.hidden_size, 128)  # First Dense layer\n",
        "        self.fc2 = nn.Linear(128, 64)  # Second Dense layer\n",
        "        self.fc3 = nn.Linear(64, self.output_size)  # Final output layer\n",
        "\n",
        "        # Activation functions\n",
        "        self.relu = nn.ReLU() # With Dense layers\n",
        "        self.sigmoid = nn.Sigmoid() # Optional: You can add a sigmoid or softmax layer if needed\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: (batch_size, sequence_length, n_features)\n",
        "        batch_size = x.size(0)\n",
        "        # Initialize hidden state\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Forward propagate GRU\n",
        "        out, _ = self.gru(x, h0)  # Shape: (batch_size, sequence_length, hidden_size)\n",
        "        # Use the last PREDICTION_LEN time steps\n",
        "        out = out[:, -PREDICTION_LEN:, :]  # Shape: (batch_size, PREDICTION_LEN, hidden_size)\n",
        "\n",
        "        # Apply fully connected layer to each time step\n",
        "        # out = self.fc(out)  # Shape: (batch_size, PREDICTION_LEN, output_size)\n",
        "\n",
        "        # Apply Dense layers\n",
        "        out = self.relu(self.fc1(out))  # Shape: (batch_size, PREDICTION_LEN, 128)\n",
        "        out = self.relu(self.fc2(out))  # Shape: (batch_size, PREDICTION_LEN, 64)\n",
        "        out = self.fc3(out)  # Shape: (batch_size, PREDICTION_LEN, output_size)\n",
        "\n",
        "        out = self.sigmoid(out) # Apply sigmoid if needed\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7R5gprtmFxTJ"
      },
      "outputs": [],
      "source": [
        "## Train the model\n",
        "def train_with_validation(model, train_loader, val_loader, criterion, optimizer, epochs=EPOCH_SIZE):\n",
        "    # Modified training with model state saving\n",
        "    best_val_loss = float('inf')\n",
        "    best_fold_state = None\n",
        "    patience = 5\n",
        "    patience_counter = 0\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    # scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "    # scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
        "\n",
        "    # Adaptive Learning Rate Adjustment\n",
        "    # prev_val_loss = float('inf')\n",
        "    # min_lr = 1e-6  # Minimum learning rate\n",
        "    # max_lr = 1e-2  # Maximum learning rate\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f'Epoch {epoch + 1}/{epochs}')\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            x_batch = x_batch.to(device, non_blocking=True)  # non_blocking=True for async transfer\n",
        "            y_batch = y_batch.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(x_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "\n",
        "            # L1 regularization\n",
        "            # l1_lambda = 1e-5\n",
        "            # l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "            # loss = loss + l1_lambda * l1_norm\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in val_loader:\n",
        "                x_batch = x_batch.to(device, non_blocking=True)  # non_blocking=True for async transfer\n",
        "                y_batch = y_batch.to(device, non_blocking=True)\n",
        "                outputs = model(x_batch)\n",
        "                val_loss += criterion(outputs, y_batch).item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        print(f'Train Loss: {avg_train_loss:.8f}, Validation Loss: {avg_val_loss:.8f}')\n",
        "\n",
        "        # Adaptive Learning Rate Adjustment\n",
        "        # new_learning_rate = 0\n",
        "        # if avg_val_loss < prev_val_loss:\n",
        "        #     # Increase learning rate by 10%\n",
        "        #     for param_group in optimizer.param_groups:\n",
        "        #         param_group['lr'] = min(param_group['lr'] * 1.1, max_lr)  # Ensure it doesn't exceed max_lr\n",
        "        #         new_learning_rate = param_group['lr']\n",
        "        #     print(f'Learning Rate: {new_learning_rate}')\n",
        "        # else:\n",
        "        #     # Decrease learning rate by 50%\n",
        "        #     for param_group in optimizer.param_groups:\n",
        "        #         param_group['lr'] = max(param_group['lr'] * 0.5, min_lr)  # Ensure it doesn't go below min_lr\n",
        "        #         new_learning_rate = param_group['lr']\n",
        "        #     print(f'Learning Rate: {new_learning_rate}')\n",
        "        # prev_val_loss = avg_val_loss\n",
        "\n",
        "        # Step the learning rate scheduler\n",
        "        # scheduler.step()\n",
        "        # print(f'Learning Rate: {scheduler.get_last_lr()[0]}')\n",
        "\n",
        "        # Early stopping logic\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_fold_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                break\n",
        "\n",
        "    return best_val_loss, best_fold_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "E_PXtkaoruSk"
      },
      "outputs": [],
      "source": [
        "def split_dates_into_folds():\n",
        "    # Get unique dates and sort them\n",
        "    unique_dates = sorted(set(pd.to_datetime(data['datetime']).dt.date))\n",
        "    total_dates = len(unique_dates)\n",
        "\n",
        "    # Calculate size of each fold\n",
        "    fold_size = total_dates // K_FOLDS\n",
        "\n",
        "    # Create non-overlapping folds\n",
        "    folds = []\n",
        "    for i in range(K_FOLDS):\n",
        "        start_idx = i * fold_size\n",
        "        end_idx = start_idx + fold_size if i < K_FOLDS - 1 else total_dates\n",
        "        fold_dates = unique_dates[start_idx:end_idx]\n",
        "        folds.append(fold_dates)\n",
        "\n",
        "    return folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uHyVTDrqq7VE"
      },
      "outputs": [],
      "source": [
        "def cross_validate():\n",
        "    full_dataset = TimeSeriesDataset(data)\n",
        "    folds = split_dates_into_folds()\n",
        "\n",
        "    best_overall_loss = float('inf')\n",
        "    best_overall_state = None\n",
        "    all_fold_losses = []  # To store losses for each fold\n",
        "\n",
        "    for fold_idx in range(K_FOLDS):\n",
        "        # Use current fold as validation, all others as training\n",
        "        val_dates = set(folds[fold_idx])\n",
        "        # Shuffle the fold indices except the current fold_idx\n",
        "        fold_indices = list(range(K_FOLDS))\n",
        "        fold_indices.remove(fold_idx)\n",
        "        random.shuffle(fold_indices)\n",
        "        train_dates = set()\n",
        "        for i in fold_indices:\n",
        "            train_dates.update(folds[i])\n",
        "\n",
        "        print(f'Fold {fold_idx + 1}:')\n",
        "        print(f'Training dates: from {min(train_dates)} to {max(train_dates)}')\n",
        "        print(f'Validation dates: from {min(val_dates)} to {max(val_dates)}')\n",
        "\n",
        "        # Split indices\n",
        "        train_indices = []\n",
        "        val_indices = []\n",
        "        for idx, (seq_indices, target_indices) in enumerate(full_dataset.valid_indices):\n",
        "            date = pd.to_datetime(data.iloc[seq_indices[0]]['datetime']).date()\n",
        "            if date in train_dates:\n",
        "                train_indices.append(idx)\n",
        "            elif date in val_dates:\n",
        "                val_indices.append(idx)\n",
        "\n",
        "        print(f'Training samples: {len(train_indices)}, Validation samples: {len(val_indices)}')\n",
        "\n",
        "        train_dataset = Subset(full_dataset, train_indices)\n",
        "        val_dataset = Subset(full_dataset, val_indices)\n",
        "\n",
        "        # Create data loaders for train and validation splits\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        # Initialize a new model, criterion, and optimizer for each fold\n",
        "        model = GRU()\n",
        "        criterion = nn.MSELoss()\n",
        "        # criterion = nn.SmoothL1Loss()  # Huber loss\n",
        "        # criterion = nn.L1Loss()\n",
        "        # def msle_loss(outputs, targets):\n",
        "        #     return torch.mean((torch.log(outputs + 1) - torch.log(targets + 1)) ** 2)\n",
        "        # criterion = msle_loss\n",
        "        # def custom_loss(outputs, targets):\n",
        "        #     mse_loss = nn.MSELoss()(outputs, targets)\n",
        "        #     mae_loss = nn.L1Loss()(outputs, targets)\n",
        "        #     return 0.7 * mse_loss + 0.3 * mae_loss\n",
        "        # criterion = custom_loss\n",
        "        optimizer = torch.optim.Adam(  # Changed from Adam to AdamW for weight_decay (L2)\n",
        "            model.parameters(),\n",
        "            lr=LEARNING_RATE,\n",
        "            # weight_decay=1e-5   # If needed, increase the weight_decay (L2) from 1e-5 to 1e-4\n",
        "        )\n",
        "\n",
        "        # Train and validate the model\n",
        "        best_fold_loss, best_fold_state = train_with_validation(\n",
        "            model,\n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            criterion,\n",
        "            optimizer\n",
        "        )\n",
        "\n",
        "        all_fold_losses.append(best_fold_loss)  # Store loss for this fold\n",
        "        print(f'Validation Loss for fold {fold_idx+1}: {best_fold_loss:.8f}')\n",
        "\n",
        "        # Save the best model\n",
        "        if best_fold_loss < best_overall_loss:\n",
        "            best_overall_loss = best_fold_loss\n",
        "            best_overall_state = best_fold_state\n",
        "\n",
        "    # Print the results\n",
        "    print('\\nCross-validation complete!')\n",
        "    print(f'Average validation loss: {np.mean(all_fold_losses):.8f}')\n",
        "    print(f'Standard deviation: {np.std(all_fold_losses):.8f}')\n",
        "\n",
        "    # Save the best model\n",
        "    torch.save(best_overall_state, '/content/drive/MyDrive/Colab Notebooks/futures.ai/gru_best_model_cv.pth')\n",
        "\n",
        "    # Plotting the validation losses\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(1, K_FOLDS + 1), all_fold_losses, marker='o', linestyle='-')\n",
        "    plt.title('Validation Loss Across Folds')\n",
        "    plt.xlabel('Fold Number')\n",
        "    plt.ylabel('Validation Loss')\n",
        "    plt.xticks(range(1, K_FOLDS + 1))\n",
        "    plt.grid()\n",
        "    plt.savefig('/content/drive/MyDrive/Colab Notebooks/futures.ai/gru_cross_validation_loss.png')  # Save the plot as an image\n",
        "    plt.close()  # Close the plot to free up memory\n",
        "\n",
        "    return best_overall_state, best_overall_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ6m93LIMOnf",
        "outputId": "f1c2f598-7c12-4b57-f74e-aa197d7f34cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique_dates: ['2016-05-26' '2016-05-27' '2016-05-31' ... '2024-05-24' '2024-05-28'\n",
            " '2024-05-29']\n",
            "unique_dates length: 2015\n",
            "self.valid_indices head: [(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
            "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
            "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]), array([50])), (array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
            "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]), array([51])), (array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
            "       19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
            "       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]), array([52])), (array([ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
            "       20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
            "       37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]), array([53])), (array([ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
            "       21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
            "       38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]), array([54])), (array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
            "       22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
            "       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]), array([55])), (array([ 6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
            "       23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39,\n",
            "       40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55]), array([56])), (array([ 7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n",
            "       24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n",
            "       41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56]), array([57])), (array([ 8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
            "       25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41,\n",
            "       42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57]), array([58])), (array([ 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,\n",
            "       26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42,\n",
            "       43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58]), array([59]))]\n",
            "self.valid_indices tail: [(array([714829, 714830, 714831, 714832, 714833, 714834, 714835, 714836,\n",
            "       714837, 714838, 714839, 714840, 714841, 714842, 714843, 714844,\n",
            "       714845, 714846, 714847, 714848, 714849, 714850, 714851, 714852,\n",
            "       714853, 714854, 714855, 714856, 714857, 714858, 714859, 714860,\n",
            "       714861, 714862, 714863, 714864, 714865, 714866, 714867, 714868,\n",
            "       714869, 714870, 714871, 714872, 714873, 714874, 714875, 714876,\n",
            "       714877, 714878]), array([714879])), (array([714830, 714831, 714832, 714833, 714834, 714835, 714836, 714837,\n",
            "       714838, 714839, 714840, 714841, 714842, 714843, 714844, 714845,\n",
            "       714846, 714847, 714848, 714849, 714850, 714851, 714852, 714853,\n",
            "       714854, 714855, 714856, 714857, 714858, 714859, 714860, 714861,\n",
            "       714862, 714863, 714864, 714865, 714866, 714867, 714868, 714869,\n",
            "       714870, 714871, 714872, 714873, 714874, 714875, 714876, 714877,\n",
            "       714878, 714879]), array([714880])), (array([714831, 714832, 714833, 714834, 714835, 714836, 714837, 714838,\n",
            "       714839, 714840, 714841, 714842, 714843, 714844, 714845, 714846,\n",
            "       714847, 714848, 714849, 714850, 714851, 714852, 714853, 714854,\n",
            "       714855, 714856, 714857, 714858, 714859, 714860, 714861, 714862,\n",
            "       714863, 714864, 714865, 714866, 714867, 714868, 714869, 714870,\n",
            "       714871, 714872, 714873, 714874, 714875, 714876, 714877, 714878,\n",
            "       714879, 714880]), array([714881])), (array([714832, 714833, 714834, 714835, 714836, 714837, 714838, 714839,\n",
            "       714840, 714841, 714842, 714843, 714844, 714845, 714846, 714847,\n",
            "       714848, 714849, 714850, 714851, 714852, 714853, 714854, 714855,\n",
            "       714856, 714857, 714858, 714859, 714860, 714861, 714862, 714863,\n",
            "       714864, 714865, 714866, 714867, 714868, 714869, 714870, 714871,\n",
            "       714872, 714873, 714874, 714875, 714876, 714877, 714878, 714879,\n",
            "       714880, 714881]), array([714882])), (array([714833, 714834, 714835, 714836, 714837, 714838, 714839, 714840,\n",
            "       714841, 714842, 714843, 714844, 714845, 714846, 714847, 714848,\n",
            "       714849, 714850, 714851, 714852, 714853, 714854, 714855, 714856,\n",
            "       714857, 714858, 714859, 714860, 714861, 714862, 714863, 714864,\n",
            "       714865, 714866, 714867, 714868, 714869, 714870, 714871, 714872,\n",
            "       714873, 714874, 714875, 714876, 714877, 714878, 714879, 714880,\n",
            "       714881, 714882]), array([714883])), (array([714834, 714835, 714836, 714837, 714838, 714839, 714840, 714841,\n",
            "       714842, 714843, 714844, 714845, 714846, 714847, 714848, 714849,\n",
            "       714850, 714851, 714852, 714853, 714854, 714855, 714856, 714857,\n",
            "       714858, 714859, 714860, 714861, 714862, 714863, 714864, 714865,\n",
            "       714866, 714867, 714868, 714869, 714870, 714871, 714872, 714873,\n",
            "       714874, 714875, 714876, 714877, 714878, 714879, 714880, 714881,\n",
            "       714882, 714883]), array([714884])), (array([714835, 714836, 714837, 714838, 714839, 714840, 714841, 714842,\n",
            "       714843, 714844, 714845, 714846, 714847, 714848, 714849, 714850,\n",
            "       714851, 714852, 714853, 714854, 714855, 714856, 714857, 714858,\n",
            "       714859, 714860, 714861, 714862, 714863, 714864, 714865, 714866,\n",
            "       714867, 714868, 714869, 714870, 714871, 714872, 714873, 714874,\n",
            "       714875, 714876, 714877, 714878, 714879, 714880, 714881, 714882,\n",
            "       714883, 714884]), array([714885])), (array([714836, 714837, 714838, 714839, 714840, 714841, 714842, 714843,\n",
            "       714844, 714845, 714846, 714847, 714848, 714849, 714850, 714851,\n",
            "       714852, 714853, 714854, 714855, 714856, 714857, 714858, 714859,\n",
            "       714860, 714861, 714862, 714863, 714864, 714865, 714866, 714867,\n",
            "       714868, 714869, 714870, 714871, 714872, 714873, 714874, 714875,\n",
            "       714876, 714877, 714878, 714879, 714880, 714881, 714882, 714883,\n",
            "       714884, 714885]), array([714886])), (array([714837, 714838, 714839, 714840, 714841, 714842, 714843, 714844,\n",
            "       714845, 714846, 714847, 714848, 714849, 714850, 714851, 714852,\n",
            "       714853, 714854, 714855, 714856, 714857, 714858, 714859, 714860,\n",
            "       714861, 714862, 714863, 714864, 714865, 714866, 714867, 714868,\n",
            "       714869, 714870, 714871, 714872, 714873, 714874, 714875, 714876,\n",
            "       714877, 714878, 714879, 714880, 714881, 714882, 714883, 714884,\n",
            "       714885, 714886]), array([714887])), (array([714838, 714839, 714840, 714841, 714842, 714843, 714844, 714845,\n",
            "       714846, 714847, 714848, 714849, 714850, 714851, 714852, 714853,\n",
            "       714854, 714855, 714856, 714857, 714858, 714859, 714860, 714861,\n",
            "       714862, 714863, 714864, 714865, 714866, 714867, 714868, 714869,\n",
            "       714870, 714871, 714872, 714873, 714874, 714875, 714876, 714877,\n",
            "       714878, 714879, 714880, 714881, 714882, 714883, 714884, 714885,\n",
            "       714886, 714887]), array([714888]))]\n",
            "self.valid_indices length: 614139\n",
            "Fold 1:\n",
            "Training dates: from 2019-01-28 to 2024-05-29\n",
            "Validation dates: from 2016-05-26 to 2019-01-25\n",
            "Training samples: 409689, Validation samples: 204450\n",
            "Epoch 1/3\n",
            "Train Loss: 0.00012794, Validation Loss: 0.00000437\n",
            "Epoch 2/3\n",
            "Train Loss: 0.00001701, Validation Loss: 0.00000437\n",
            "Epoch 3/3\n",
            "Train Loss: 0.00001701, Validation Loss: 0.00000445\n",
            "Validation Loss for fold 1: 0.00000437\n",
            "Fold 2:\n",
            "Training dates: from 2016-05-26 to 2024-05-29\n",
            "Validation dates: from 2019-01-28 to 2021-09-23\n",
            "Training samples: 409688, Validation samples: 204451\n",
            "Epoch 1/3\n",
            "Train Loss: 0.00007477, Validation Loss: 0.00001368\n",
            "Epoch 2/3\n",
            "Train Loss: 0.00001241, Validation Loss: 0.00001372\n",
            "Epoch 3/3\n",
            "Train Loss: 0.00001240, Validation Loss: 0.00001372\n",
            "Validation Loss for fold 2: 0.00001368\n",
            "Fold 3:\n",
            "Training dates: from 2016-05-26 to 2021-09-23\n",
            "Validation dates: from 2021-09-24 to 2024-05-29\n",
            "Training samples: 408901, Validation samples: 205238\n",
            "Epoch 1/3\n",
            "Train Loss: 0.00012740, Validation Loss: 0.00002034\n",
            "Epoch 2/3\n",
            "Train Loss: 0.00000904, Validation Loss: 0.00002031\n",
            "Epoch 3/3\n",
            "Train Loss: 0.00000903, Validation Loss: 0.00002034\n",
            "Validation Loss for fold 3: 0.00002031\n",
            "\n",
            "Cross-validation complete!\n",
            "Average validation loss: 0.00001279\n",
            "Standard deviation: 0.00000654\n",
            "\n",
            "Best model saved with validation loss: 0.00000437\n"
          ]
        }
      ],
      "source": [
        "# Perform cross-validation\n",
        "best_overall_state, best_overall_loss = cross_validate()\n",
        "\n",
        "# Create a final model with the best weights\n",
        "# final_model = ANN()\n",
        "# final_model.load_state_dict(best_overall_state)\n",
        "\n",
        "print(f'\\nBest model saved with validation loss: {best_overall_loss:.8f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}