{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kICVgG6H1iu0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cc524e8-9006-4c85-9bdc-53c043f77d0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime, time\n",
        "\n",
        "MIN_SHADOW_PROPORTION = 0.0001  # 0.0005"
      ],
      "metadata": {
        "id": "Owe53iLZ_BeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume that the UNIX time key range for the data is 0930 to 1600 UTC, and does not need to fit into a US time zone, either EST or EDT.\n",
        "\n",
        "# Load JSON data\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/futures.ai/spy_1min_regularhours.json', 'r') as json_file:\n",
        "    json_data = json.load(json_file)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(json_data)\n",
        "\n",
        "# Calculate EMA\n",
        "def calculate_ema(df, period):\n",
        "    return df['close'].ewm(span=period, adjust=False).mean()\n",
        "\n",
        "# Revised RSI calculation to uses EMA\n",
        "def calculate_rsi(df, period=7):\n",
        "    delta = df['close'].diff()\n",
        "\n",
        "    # Separate gains and losses\n",
        "    gains = delta.where(delta > 0, 0.0)\n",
        "    losses = -delta.where(delta < 0, 0.0)\n",
        "\n",
        "    # Calculate EMA multiplier\n",
        "    alpha = 2 / (period + 1)\n",
        "\n",
        "    # First averages (SMA)\n",
        "    first_avg_gain = gains.iloc[:period].mean() if period <= len(gains) else gains.mean()\n",
        "    first_avg_loss = losses.iloc[:period].mean() if period <= len(losses) else losses.mean()\n",
        "\n",
        "    # Initialize series\n",
        "    avg_gains = pd.Series(index=delta.index)\n",
        "    avg_losses = pd.Series(index=delta.index)\n",
        "\n",
        "    # Set first values\n",
        "    avg_gains.iloc[period-1] = first_avg_gain\n",
        "    avg_losses.iloc[period-1] = first_avg_loss\n",
        "\n",
        "    # Calculate EMA\n",
        "    for i in range(period, len(gains)):\n",
        "        avg_gains.iloc[i] = alpha * gains.iloc[i] + (1 - alpha) * avg_gains.iloc[i-1]\n",
        "        avg_losses.iloc[i] = alpha * losses.iloc[i] + (1 - alpha) * avg_losses.iloc[i-1]\n",
        "\n",
        "    rs = avg_gains / np.where(avg_losses == 0, 1e-10, avg_losses)\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "\n",
        "    # Ensure RSI stays within bounds\n",
        "    rsi = np.clip(rsi, 0, 100)\n",
        "\n",
        "    return avg_gains, avg_losses, rsi\n",
        "\n",
        "# Append EMA to DataFrame\n",
        "df['EMA_5'] = calculate_ema(df, 5)\n",
        "df['EMA_10'] = calculate_ema(df, 10)\n",
        "df['EMA_15'] = calculate_ema(df, 15)\n",
        "df['EMA_20'] = calculate_ema(df, 20)\n",
        "\n",
        "# Calculate RSI and gain/loss\n",
        "df['Gain'], df['Loss'], df['RSI'] = calculate_rsi(df)\n",
        "\n",
        "# Drop the first 20 rows (EMA 20)\n",
        "df = df.iloc[20:].reset_index(drop=True)\n",
        "\n",
        "# Check for NaN values in the DataFrame\n",
        "if df.isna().any().any():\n",
        "    print(\"Error: NaN values found in calculations.\")\n",
        "    problematic_rows = df[df.isna().any(axis=1)]\n",
        "    print(problematic_rows[['time_key', 'close', 'RSI']])  # Print only relevant columns\n",
        "\n",
        "    # Additional diagnostics\n",
        "    print(\"Close prices for problematic rows:\")\n",
        "    for index in problematic_rows.index:\n",
        "        print(f\"Index: {index}, Close Price: {df.at[index, 'close']}, Gain: {df.at[index, 'Gain']}, Loss: {df.at[index, 'Loss']}\")\n",
        "\n",
        "# Count rows before dropping\n",
        "initial_row_count = df.shape[0]\n",
        "\n",
        "# Drop rows where any of the indicators are NaN\n",
        "df = df.dropna(subset=['EMA_5', 'EMA_10', 'EMA_15', 'EMA_20', 'RSI']).reset_index(drop=True)\n",
        "\n",
        "# Count rows after dropping\n",
        "final_row_count = df.shape[0]\n",
        "dropped_rows_count = initial_row_count - final_row_count\n",
        "\n",
        "# Print the number of dropped rows\n",
        "print(f\"Number of rows dropped due to NaN values: {dropped_rows_count}\")\n",
        "\n",
        "# Convert time_key to datetime\n",
        "df['datetime'] = pd.to_datetime(df['time_key'], unit='s')\n",
        "\n",
        "# Filter out days that do not start before 9:35 AM\n",
        "df['date'] = df['datetime'].dt.date\n",
        "# Convert '09:35:00' to a datetime object for comparison with datetime\n",
        "comparison_time = pd.to_datetime('09:35:00').time()\n",
        "# Check the minimum time for each date and compare\n",
        "valid_days = df.groupby('date')['datetime'].min().dt.time < comparison_time\n",
        "valid_dates = valid_days[valid_days].index\n",
        "# Filter the DataFrame to keep only valid dates\n",
        "df = df[df['date'].isin(valid_dates)].copy()\n",
        "\n",
        "# Filter function for trading hours\n",
        "def is_valid_trading_time(dt):\n",
        "    # Keep data between 10:00 AM and 3:55 PM\n",
        "    return time(10, 0) <= dt.time() <= time(15, 55) # For 3 minute interval: time(10, 30) <= t <= time(15, 55)\n",
        "\n",
        "# Apply time filter directly on the datetime column\n",
        "df = df[df['datetime'].apply(is_valid_trading_time)].copy()\n",
        "\n",
        "# Transform RSI to Int values based on specified rules\n",
        "def transform_rsi_to_int(rsi):\n",
        "    if rsi >= 70:\n",
        "        return 1\n",
        "    elif rsi <= 30:\n",
        "        return -1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "df['RSI_INT'] = df['RSI'].apply(transform_rsi_to_int)\n",
        "\n",
        "# Add EMA comparison columns\n",
        "df['EMA_5_EMA_10'] = (df['EMA_5'] > df['EMA_10']).astype(int)\n",
        "df['EMA_15_EMA_20'] = (df['EMA_15'] > df['EMA_20']).astype(int)\n",
        "\n",
        "# Calculate shadow lengths\n",
        "def calculate_shadow_fold(row):\n",
        "    # body_size = abs(row['open'] - row['close'])\n",
        "    min_body_threshold = row['close'] * MIN_SHADOW_PROPORTION\n",
        "\n",
        "    if row['open'] > row['close']:  # Bearish candle\n",
        "        upper_shadow = row['high'] - row['open']\n",
        "        lower_shadow = row['close'] - row['low']\n",
        "    else:  # Bullish candle\n",
        "        upper_shadow = row['high'] - row['close']\n",
        "        lower_shadow = row['open'] - row['low']\n",
        "\n",
        "    if upper_shadow < min_body_threshold:\n",
        "        upper_fold = 0\n",
        "    else:\n",
        "        upper_fold = upper_shadow / min_body_threshold\n",
        "\n",
        "    if lower_shadow < min_body_threshold:\n",
        "        lower_fold = 0\n",
        "    else:\n",
        "        lower_fold = lower_shadow / min_body_threshold\n",
        "\n",
        "    return upper_fold, lower_fold\n",
        "\n",
        "# Calculate shadow folds for each row\n",
        "shadow_folds = df.apply(calculate_shadow_fold, axis=1)\n",
        "upper_folds, lower_folds = zip(*shadow_folds)\n",
        "\n",
        "# Convert folds to categorical values (0, 1, 2, etc.)\n",
        "def categorize_fold(fold):\n",
        "    if fold < 1:\n",
        "        return 0\n",
        "    return int(fold)\n",
        "\n",
        "df['LONG_UPPER_SHADOW'] = [categorize_fold(fold) for fold in upper_folds]\n",
        "df['LONG_LOWER_SHADOW'] = [categorize_fold(fold) for fold in lower_folds]\n",
        "\n",
        "# Drop the temporary columns\n",
        "# df = df.drop(['open', 'high', 'low'], axis=1)\n",
        "\n",
        "# Convert 'datetime' to ISO format (string)\n",
        "df['datetime'] = df['datetime'].dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
        "\n",
        "# Retain fields along with calculated indicators\n",
        "result = df[['datetime', 'close', 'LONG_UPPER_SHADOW', 'LONG_LOWER_SHADOW', 'EMA_5', 'EMA_10', 'EMA_15', 'EMA_20', 'EMA_5_EMA_10', 'EMA_15_EMA_20', 'RSI', 'RSI_INT']].copy() #'open', 'high', 'low',\n",
        "\n",
        "# Convert to dictionary format for JSON\n",
        "result = result.to_dict(orient='records')\n",
        "\n",
        "# Save to a new JSON file\n",
        "# with open('/content/drive/MyDrive/Colab Notebooks/futures.ai/spy_1min_regularhours_truncated_preprocessed.json', 'w') as json_file:\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/futures.ai/spy_1min_regularhours_truncated_semisupervised_preprocessed.json', 'w') as json_file:\n",
        "    json.dump(result, json_file)"
      ],
      "metadata": {
        "id": "7RMdnYUZ1SrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81163a83-1694-4e04-8f8f-4b7b7159a168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows dropped due to NaN values: 0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}