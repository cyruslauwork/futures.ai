{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FARZM7X0-y_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "000c2972-13c2-443c-8fa0-411e799d8707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to check and install specific package versions\n",
        "def install_package(package, version):\n",
        "    try:\n",
        "        # Check the current version\n",
        "        current_version = __import__(package).__version__\n",
        "\n",
        "        # Special handling for torch to check for version with CUDA suffix\n",
        "        if package == \"torch\":\n",
        "            if current_version.startswith(version):\n",
        "                print(f\"{package} version {version} is already installed.\")\n",
        "                return\n",
        "        else:\n",
        "            if current_version == version:\n",
        "                print(f\"{package} version {version} is already installed.\")\n",
        "                return\n",
        "\n",
        "        print(f\"Uninstalling {package} version {current_version}...\")\n",
        "        !pip uninstall -y {package}\n",
        "        print(f\"Installing {package} version {version}...\")\n",
        "        !pip install {package}=={version}\n",
        "\n",
        "    except ImportError:\n",
        "        print(f\"{package} not installed. Installing {package} version {version}...\")\n",
        "        !pip install {package}=={version}\n",
        "\n",
        "# Specify the packages and their desired versions\n",
        "packages = {\n",
        "    \"pandas\": \"2.2.2\",\n",
        "    \"numpy\": \"1.26.4\",\n",
        "    \"scikit-learn\": \"1.6.0\",\n",
        "    \"torch\": \"2.5.1\",\n",
        "    \"joblib\": \"1.4.2\",\n",
        "    \"matplotlib\": \"3.10.0\"\n",
        "}\n",
        "\n",
        "# Check and install packages\n",
        "for pkg, ver in packages.items():\n",
        "    install_package(pkg, ver)\n",
        "\n",
        "# Import the necessary libraries after ensuring correct versions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "import joblib\n",
        "import matplotlib as mpl  # Import matplotlib for version checking\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, time\n",
        "import random\n",
        "from sklearn import __version__ as sklearn_version  # Import sklearn version\n",
        "\n",
        "# Print package versions\n",
        "print(\"pandas version:\", pd.__version__)\n",
        "print(\"numpy version:\", np.__version__)\n",
        "print(\"scikit-learn version:\", sklearn_version)\n",
        "print(\"torch version:\", torch.__version__)\n",
        "print(\"joblib version:\", joblib.__version__)\n",
        "print(\"matplotlib version:\", mpl.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "hktEy2stK4Vo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "638784df-c0d8-4a62-cb34-5967acf345f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pandas version 2.2.2 is already installed.\n",
            "numpy version 1.26.4 is already installed.\n",
            "scikit-learn not installed. Installing scikit-learn version 1.6.0...\n",
            "Collecting scikit-learn==1.6.0\n",
            "  Downloading scikit_learn-1.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.0) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.0) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.0) (3.5.0)\n",
            "Downloading scikit_learn-1.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "Successfully installed scikit-learn-1.6.0\n",
            "torch version 2.5.1 is already installed.\n",
            "joblib version 1.4.2 is already installed.\n",
            "matplotlib version 3.10.0 is already installed.\n",
            "pandas version: 2.2.2\n",
            "numpy version: 1.26.4\n",
            "scikit-learn version: 1.6.0\n",
            "torch version: 2.5.1+cu124\n",
            "joblib version: 1.4.2\n",
            "matplotlib version: 3.10.0\n",
            "CUDA available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DETREND_WINDOW_SIZE = 2\n",
        "BATCH_SIZE = 2048\n",
        "WINDOW_SIZE = 75\n",
        "LEARNING_RATE = 0.0005  # Changed from 0.001 to 0.0005\n",
        "EPOCH_SIZE = 3 # 50\n",
        "PREDICTION_LEN = 1   # Number of time steps to predict each time\n",
        "K_FOLDS = 3 # 10  # Number of folds for cross-validation\n",
        "TIME_INTERVAL_SEC = 60  # For 1 minute interval: 60\n",
        "PREDICTION_TIMESTEP_SPAN = 0  # 14"
      ],
      "metadata": {
        "id": "SJl7Rb-SK8Eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the JSON data\n",
        "# data = pd.read_json('/content/drive/MyDrive/Colab Notebooks/futures.ai/spy_1min_regularhours_truncated_preprocessed.json')\n",
        "data = pd.read_json('/content/drive/MyDrive/Colab Notebooks/futures.ai/spy_1min_regularhours_truncated_semisupervised_preprocessed.json')\n",
        "\n",
        "# Convert 'datetime' strings back to datetime objects\n",
        "data['datetime'] = pd.to_datetime(data['datetime'])\n",
        "\n",
        "## De-trend the data\n",
        "#\n",
        "# De-trending helps to remove any long-term trends or seasonality from the data,\n",
        "# allowing the model to focus on the fluctuations that matter most for prediction.\n",
        "#\n",
        "# Why De-trend Before Normalization?\n",
        "# Focus on Fluctuations:\n",
        "# Normalizing data with trends can make it difficult for the model to learn meaningful patterns,\n",
        "# as the model may focus on the trends rather than the actual relationships between features.\n",
        "# Consistent Scale:\n",
        "# By de-trending all relevant features, you ensure that they are on a similar scale, which can improve the learning process.\n",
        "FEATURES_TO_DETREND = ['close', 'EMA_5', 'EMA_10', 'EMA_15', 'EMA_20']  #'open', 'high', 'low',\n",
        "FEATURES_NOT_TO_DETREND = ['LONG_UPPER_SHADOW', 'LONG_LOWER_SHADOW', 'EMA_5_EMA_10', 'EMA_15_EMA_20', 'RSI', 'RSI_INT']\n",
        "# FEATURES_NOT_TO_DETREND = ['RSI']\n",
        "ALL_FEATURES = FEATURES_TO_DETREND + FEATURES_NOT_TO_DETREND\n",
        "\n",
        "for feature in FEATURES_TO_DETREND:\n",
        "    # SMA for de-trending\n",
        "    #\n",
        "    # Choosing the Window Size for Rolling Mean\n",
        "    #\n",
        "    # If your data has short-term fluctuations, a smaller DETREND_WINDOW_SIZE (e.g., 5-10)\n",
        "    # might be more appropriate, allowing you to capture more immediate trends.\n",
        "    # For longer-term trends, a larger DETREND_WINDOW_SIZE (e.g., 30-60) is often used\n",
        "    # to smooth out more significant fluctuations and focus on overarching trends.\n",
        "    #\n",
        "    # Consider what you are trying to predict. If you are interested in short-term price movements,\n",
        "    # a smaller window might be better. For long-term predictions, a larger window can help smooth out noise.\n",
        "    #\n",
        "    # If your goal is to analyze short-term trends without losing the fluctuations, you might consider using a very short window size (e.g., 2 or 3)\n",
        "    # for a moving average. This would still smooth the data slightly but would retain more of the original fluctuations compared to larger windows.\n",
        "    # data[f'{feature}_trend'] = data[feature].rolling(window=DETREND_WINDOW_SIZE).mean()  # Simple moving average\n",
        "    # data[f'{feature}_detrended'] = data[feature] - data[f'{feature}_trend']\n",
        "\n",
        "    # Differencing for de-trending\n",
        "    data[f'{feature}_detrended'] = data[feature].diff()  # Calculate the difference from the previous value\n",
        "data = data.dropna()    # Drop the first row which will be NaN due to differencing or drop NaN values created by rolling mean\n",
        "\n",
        "## Normalize the de-trended features\n",
        "\n",
        "# Define fixed scaler for RSI\n",
        "class FixedRangeScaler:\n",
        "    def __init__(self, feature_range=(0, 1), input_range=(0, 100)):\n",
        "        self.feature_range = feature_range\n",
        "        self.input_range = input_range\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_std = (X - self.input_range[0]) / (self.input_range[1] - self.input_range[0])\n",
        "        X_scaled = X_std * (self.feature_range[1] - self.feature_range[0]) + self.feature_range[0]\n",
        "        return X_scaled\n",
        "\n",
        "    def inverse_transform(self, X):\n",
        "        X_std = (X - self.feature_range[0]) / (self.feature_range[1] - self.feature_range[0])\n",
        "        X_original = X_std * (self.input_range[1] - self.input_range[0]) + self.input_range[0]\n",
        "        return X_original\n",
        "\n",
        "# Initialize scalers\n",
        "dynamic_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "long_upper_shadow_dynamic_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "long_lower_shadow_dynamic_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "rsi_scaler = FixedRangeScaler(feature_range=(0, 1), input_range=(0, 100))\n",
        "rsi_int_scaler = FixedRangeScaler(feature_range=(0, 1), input_range=(-1, 1))\n",
        "\n",
        "data = data.copy()  # Create a copy to avoid SettingWithCopyWarning\n",
        "data[[f'{feature}_normalized' for feature in FEATURES_TO_DETREND]] = dynamic_scaler.fit_transform(\n",
        "    data[[f'{feature}_detrended' for feature in FEATURES_TO_DETREND]].values\n",
        ")\n",
        "data['LONG_UPPER_SHADOW_normalized'] = long_upper_shadow_dynamic_scaler.fit_transform(data['LONG_UPPER_SHADOW'].values.reshape(-1, 1))\n",
        "data['LONG_LOWER_SHADOW_normalized'] = long_lower_shadow_dynamic_scaler.fit_transform(data['LONG_LOWER_SHADOW'].values.reshape(-1, 1))\n",
        "data['EMA_5_EMA_10_normalized'] = data['EMA_5_EMA_10']\n",
        "data['EMA_15_EMA_20_normalized'] = data['EMA_15_EMA_20']\n",
        "data['RSI_normalized'] = rsi_scaler.transform(data['RSI'].values.reshape(-1, 1))\n",
        "data['RSI_INT_normalized'] = rsi_int_scaler.transform(data['RSI_INT'].values.reshape(-1, 1))\n",
        "\n",
        "# Save the normalized and de-trended data for future use\n",
        "# data.to_json('processed_data.json', orient='records')\n",
        "\n",
        "# Save the scalers\n",
        "joblib.dump(dynamic_scaler, '/content/drive/MyDrive/Colab Notebooks/futures.ai/dynamic_scaler.pkl')\n",
        "joblib.dump(long_upper_shadow_dynamic_scaler, '/content/drive/MyDrive/Colab Notebooks/futures.ai/long_upper_shadow_dynamic_scaler.pkl')\n",
        "joblib.dump(long_lower_shadow_dynamic_scaler, '/content/drive/MyDrive/Colab Notebooks/futures.ai/long_lower_shadow_dynamic_scaler.pkl')\n",
        "joblib.dump(rsi_scaler, '/content/drive/MyDrive/Colab Notebooks/futures.ai/rsi_scaler.pkl')\n",
        "joblib.dump(rsi_int_scaler, '/content/drive/MyDrive/Colab Notebooks/futures.ai/rsi_int_scaler.pkl')\n",
        "\n",
        "# Fit a dedicated scaler just for 'close'\n",
        "close_dynamic_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "data['close_normalized'] = close_dynamic_scaler.fit_transform(data[['close_detrended']].values)\n",
        "\n",
        "# Save the close scaler\n",
        "joblib.dump(close_dynamic_scaler, '/content/drive/MyDrive/Colab Notebooks/futures.ai/close_dynamic_scaler.pkl')"
      ],
      "metadata": {
        "id": "mylLY_yiLUwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "635d1b7e-8af6-4bc7-d367-1c838999ae51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Colab Notebooks/futures.ai/close_dynamic_scaler.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Define a PyTorch Dataset\n",
        "# Modified TimeSeriesDataset with strict day boundary checks for day-after-day training\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.sequence_length = WINDOW_SIZE\n",
        "        self.prediction_length = PREDICTION_LEN\n",
        "\n",
        "        # Convert the 'datetime' strings to numpy datetime64 for efficient processing\n",
        "        self.dates = np.array([np.datetime64(dt) for dt in data['datetime']])\n",
        "\n",
        "        # Extract features as numpy arrays\n",
        "        self.features = np.stack([\n",
        "            data[f'{feature}_normalized'].values\n",
        "            for feature in ALL_FEATURES\n",
        "        ], axis=1)  # Shape: (n_samples, n_features)\n",
        "\n",
        "        # Store indices for each day\n",
        "        unique_dates = np.unique(self.dates.astype('datetime64[D]'))\n",
        "        print(f'unique_dates: {unique_dates}')\n",
        "        print(f'unique_dates length: {len(unique_dates)}')\n",
        "        self.daily_indices = {\n",
        "            date: np.where(self.dates.astype('datetime64[D]') == date)[0] for date in unique_dates\n",
        "        }\n",
        "\n",
        "        # Pre-compute valid indices for each day\n",
        "        self.valid_indices = []\n",
        "        for date, day_indices in self.daily_indices.items():\n",
        "            if len(day_indices) == 0:\n",
        "                continue  # Skip if there are no indices for this date\n",
        "\n",
        "            for i in range(len(day_indices)):\n",
        "                seq_start = i\n",
        "                seq_end = i + self.sequence_length\n",
        "                target_start = seq_end + PREDICTION_TIMESTEP_SPAN\n",
        "                target_end = target_start + self.prediction_length\n",
        "\n",
        "                # Check if all indices are within the same day\n",
        "                if target_end <= len(day_indices):\n",
        "                    # Verify time continuity\n",
        "                    if self._verify_time_continuity(day_indices, seq_start, seq_end, target_start, target_end):\n",
        "                        self.valid_indices.append((\n",
        "                            day_indices[seq_start:seq_end],\n",
        "                            day_indices[target_start:target_end]\n",
        "                        ))\n",
        "        print(f'self.valid_indices head: {self.valid_indices[:10]}')\n",
        "        print(f'self.valid_indices tail: {self.valid_indices[-10:]}')\n",
        "        print(f'self.valid_indices length: {len(self.valid_indices)}')\n",
        "\n",
        "    def _verify_time_continuity(self, day_indices, seq_start, seq_end, target_start, target_end):\n",
        "        # Verify that times are continuous\n",
        "        for j in range(seq_start, seq_end - 1):\n",
        "            t1 = self.dates[day_indices[j]]\n",
        "            t2 = self.dates[day_indices[j + 1]]\n",
        "            if (t2 - t1).astype('timedelta64[s]') != TIME_INTERVAL_SEC:\n",
        "                return False\n",
        "\n",
        "        for j in range(target_start, target_end - 1):\n",
        "            t1 = self.dates[day_indices[j]]\n",
        "            t2 = self.dates[day_indices[j + 1]]\n",
        "            if (t2 - t1).astype('timedelta64[s]') != TIME_INTERVAL_SEC:\n",
        "                return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def __len__(self):\n",
        "        # Total number of items across all days\n",
        "        return len(self.valid_indices)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        seq_indices, target_indices = self.valid_indices[index]\n",
        "\n",
        "        # Return data in (features, sequence) format\n",
        "\n",
        "        # Get input sequence\n",
        "        x = self.features[seq_indices]  # Shape: (sequence_length, n_features)\n",
        "        x = torch.tensor(x, dtype=torch.float32)\n",
        "\n",
        "        # Get target sequence\n",
        "        # Change in target extraction\n",
        "        # y = self.features[target_indices]   # Target is now all normalized features for the next timestep(s)\n",
        "        # y = torch.tensor(y, dtype=torch.float32)  # Shape: (prediction_length, n_features)\n",
        "        y = self.features[target_indices][:, 0]  # Only extracting the 'close' price\n",
        "        y = torch.tensor(y, dtype=torch.float32).view(-1, 1)  # Shape: (prediction_length, 1)\n",
        "\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "-yZX991GL_Mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding regularization can help prevent overfitting in your neural network,\n",
        "# especially when you have a more complex model or when your dataset is small\n",
        "#\n",
        "# L1 (Lasso) adds a penalty equal to the absolute value of the magnitude of coefficients (weights) to the loss function.\n",
        "# This can help with feature selection and simplify the model.\n",
        "#\n",
        "# L2 (Ridge) adds a penalty to the loss function based on the size of the weights.\n",
        "# This can help keep the weights small and prevent overfitting.\n",
        "#\n",
        "# Using both L1 and L2 (often referred to as Elastic Net when combined) can leverage the advantages of both methods.\n",
        "# L1 helps with feature selection, while L2 helps stabilize and improve the performance of the model by controlling the overall weight magnitude.\n",
        "# Combining these techniques can lead to better generalization on unseen data.\n",
        "# L1 helps to simplify the model, while L2 helps ensure the remaining features are well-weighted and not overly influenced by noise.\n",
        "#\n",
        "# Dropout randomly sets a fraction of input units to 0 during training,\n",
        "# which prevents the network from becoming too reliant on any one feature and promotes redundancy in the network.\n",
        "#\n",
        "# Batch Normalization, a technique used to stabilize and accelerate training by normalizing the inputs to each layer.\n",
        "# It helps maintain the mean and variance of activations within a certain range, making the training process more stable.\n",
        "\n",
        "## Define a simple ANN model with three layers\n",
        "class ANN(nn.Module):\n",
        "    # The BATCH_SIZE is a parameter used during training and data loading, while the model itself is defined independently of\n",
        "    # how many samples are processed at once. The model should be able to handle varying batch sizes dynamically during training and inference.\n",
        "    def __init__(self):\n",
        "        super(ANN, self).__init__()\n",
        "        self.input_size = WINDOW_SIZE * len(ALL_FEATURES) # Input Layer\n",
        "        # self.output_size = len(ALL_FEATURES) * PREDICTION_LEN # Output Layer with all normalized features\n",
        "        self.output_size = PREDICTION_LEN # Output Layer with only the 'close' price\n",
        "\n",
        "        # Modified to work with batch_first=True\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(),  # Flattens input while preserving batch dimension\n",
        "\n",
        "            # First Hidden Layer\n",
        "            nn.Linear(self.input_size, 256),\n",
        "            # nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(0.2),\n",
        "            # nn.Linear(256, self.output_size),\n",
        "\n",
        "            # Second Hidden Layer\n",
        "            nn.Linear(256, 128),\n",
        "            # nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(0.2),\n",
        "            nn.Linear(128, self.output_size),\n",
        "\n",
        "            # Third hidden layer\n",
        "            # nn.Linear(128, 64),\n",
        "            # nn.BatchNorm1d(64),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Dropout(0.2),\n",
        "            # nn.Linear(64, self.output_size),\n",
        "\n",
        "            nn.Sigmoid()  # Add this if you want outputs between 0 and 1\n",
        "        )\n",
        "        # The choice of layer sizes (256 and 128) is somewhat arbitrary and depends on the design of the neural network.\n",
        "        # Typically, you start with a larger number of neurons (256) in the first hidden layer to capture more complex patterns\n",
        "        # and then reduce the number of neurons in subsequent layers (128) to refine those patterns.\n",
        "        # If you reversed them (using 128 for the first layer and 256 for the second), the network could still work,\n",
        "        # but it might not learn as effectively due to having fewer neurons in the first layer to capture the initial complexity.\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: (batch_size, sequence_length, n_features)\n",
        "        batch_size = x.size(0)\n",
        "        output = self.model(x.reshape(batch_size, -1))\n",
        "        # Reshape to (batch_size, prediction_length, n_features)\n",
        "        return output.view(batch_size, PREDICTION_LEN, -1)\n",
        "        # batch_size = x.size(0) refers to the number of samples in the current batch. This may or may not equal BATCH_SIZE,\n",
        "        # depending on the specific batch processed during training. If the last batch is smaller than BATCH_SIZE,\n",
        "        # then batch_size will be less than BATCH_SIZE."
      ],
      "metadata": {
        "id": "WTKpIgl6MEEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R5gprtmFxTJ"
      },
      "outputs": [],
      "source": [
        "## Train the model\n",
        "def train_with_validation(model, train_loader, val_loader, criterion, optimizer, epochs=EPOCH_SIZE):\n",
        "    # Modified training with model state saving\n",
        "    best_val_loss = float('inf')\n",
        "    best_fold_state = None\n",
        "    patience = 5\n",
        "    patience_counter = 0\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f'Epoch {epoch + 1}/{epochs}')\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            x_batch = x_batch.to(device, non_blocking=True)  # non_blocking=True for async transfer\n",
        "            y_batch = y_batch.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(x_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "\n",
        "            # L1 regularization\n",
        "            # l1_lambda = 1e-5\n",
        "            # l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "            # loss = loss + l1_lambda * l1_norm\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in val_loader:\n",
        "                x_batch = x_batch.to(device, non_blocking=True)  # non_blocking=True for async transfer\n",
        "                y_batch = y_batch.to(device, non_blocking=True)\n",
        "                outputs = model(x_batch)\n",
        "                val_loss += criterion(outputs, y_batch).item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        print(f'Train Loss: {avg_train_loss:.8f}, Validation Loss: {avg_val_loss:.8f}')\n",
        "\n",
        "        # Early stopping logic\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_fold_state = model.state_dict()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                break\n",
        "\n",
        "    return best_val_loss, best_fold_state"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dates_into_folds():\n",
        "    # Get unique dates and sort them\n",
        "    unique_dates = sorted(set(pd.to_datetime(data['datetime']).dt.date))\n",
        "    total_dates = len(unique_dates)\n",
        "\n",
        "    # Calculate size of each fold\n",
        "    fold_size = total_dates // K_FOLDS\n",
        "\n",
        "    # Create non-overlapping folds\n",
        "    folds = []\n",
        "    for i in range(K_FOLDS):\n",
        "        start_idx = i * fold_size\n",
        "        end_idx = start_idx + fold_size if i < K_FOLDS - 1 else total_dates\n",
        "        fold_dates = unique_dates[start_idx:end_idx]\n",
        "        folds.append(fold_dates)\n",
        "\n",
        "    return folds"
      ],
      "metadata": {
        "id": "E_PXtkaoruSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_validate():\n",
        "    full_dataset = TimeSeriesDataset(data)\n",
        "    folds = split_dates_into_folds()\n",
        "\n",
        "    best_overall_loss = float('inf')\n",
        "    best_overall_state = None\n",
        "    all_fold_losses = []  # To store losses for each fold\n",
        "\n",
        "    for fold_idx in range(K_FOLDS):\n",
        "        # Use current fold as validation, all others as training\n",
        "        val_dates = set(folds[fold_idx])\n",
        "        # Shuffle the fold indices except the current fold_idx\n",
        "        fold_indices = list(range(K_FOLDS))\n",
        "        fold_indices.remove(fold_idx)\n",
        "        random.shuffle(fold_indices)\n",
        "        train_dates = set()\n",
        "        for i in fold_indices:\n",
        "            train_dates.update(folds[i])\n",
        "\n",
        "        print(f'Fold {fold_idx + 1}:')\n",
        "        print(f'Training dates: from {min(train_dates)} to {max(train_dates)}')\n",
        "        print(f'Validation dates: from {min(val_dates)} to {max(val_dates)}')\n",
        "\n",
        "        # Split indices\n",
        "        train_indices = []\n",
        "        val_indices = []\n",
        "        for idx, (seq_indices, target_indices) in enumerate(full_dataset.valid_indices):\n",
        "            date = pd.to_datetime(data.iloc[seq_indices[0]]['datetime']).date()\n",
        "            if date in train_dates:\n",
        "                train_indices.append(idx)\n",
        "            elif date in val_dates:\n",
        "                val_indices.append(idx)\n",
        "\n",
        "        print(f'Training samples: {len(train_indices)}, Validation samples: {len(val_indices)}')\n",
        "\n",
        "        train_dataset = Subset(full_dataset, train_indices)\n",
        "        val_dataset = Subset(full_dataset, val_indices)\n",
        "\n",
        "        # Create data loaders for train and validation splits\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        # Initialize a new model, criterion, and optimizer for each fold\n",
        "        model = ANN()\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = torch.optim.Adam(  # Changed from Adam to AdamW for weight_decay (L2)\n",
        "            model.parameters(),\n",
        "            lr=LEARNING_RATE,\n",
        "            # weight_decay=1e-5   # If needed, increase the weight_decay (L2) from 1e-5 to 1e-4\n",
        "        )\n",
        "\n",
        "        # Train and validate the model\n",
        "        best_fold_loss, best_fold_state = train_with_validation(\n",
        "            model,\n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            criterion,\n",
        "            optimizer\n",
        "        )\n",
        "\n",
        "        all_fold_losses.append(best_fold_loss)  # Store loss for this fold\n",
        "        print(f'Validation Loss for fold {fold_idx+1}: {best_fold_loss:.8f}')\n",
        "\n",
        "        # Save the best model\n",
        "        if best_fold_loss < best_overall_loss:\n",
        "            best_overall_loss = best_fold_loss\n",
        "            best_overall_state = best_fold_state\n",
        "\n",
        "    # Print the results\n",
        "    print('\\nCross-validation complete!')\n",
        "    print(f'Average validation loss: {np.mean(all_fold_losses):.8f}')\n",
        "    print(f'Standard deviation: {np.std(all_fold_losses):.8f}')\n",
        "\n",
        "    # Save the best model\n",
        "    torch.save(best_overall_state, '/content/drive/MyDrive/Colab Notebooks/futures.ai/best_model_cv.pth')\n",
        "\n",
        "    # Plotting the validation losses\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(1, K_FOLDS + 1), all_fold_losses, marker='o', linestyle='-')\n",
        "    plt.title('Validation Loss Across Folds')\n",
        "    plt.xlabel('Fold Number')\n",
        "    plt.ylabel('Validation Loss')\n",
        "    plt.xticks(range(1, K_FOLDS + 1))\n",
        "    plt.grid()\n",
        "    plt.savefig('/content/drive/MyDrive/Colab Notebooks/futures.ai/cross_validation_loss.png')  # Save the plot as an image\n",
        "    plt.close()  # Close the plot to free up memory\n",
        "\n",
        "    return best_overall_state, best_overall_loss"
      ],
      "metadata": {
        "id": "uHyVTDrqq7VE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform cross-validation\n",
        "best_overall_state, best_overall_loss = cross_validate()\n",
        "\n",
        "# Create a final model with the best weights\n",
        "# final_model = ANN()\n",
        "# final_model.load_state_dict(best_overall_state)\n",
        "\n",
        "print(f'\\nBest model saved with validation loss: {best_overall_loss:.8f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ6m93LIMOnf",
        "outputId": "7ea744da-793d-4803-8118-647f061707ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique_dates: ['2016-05-26' '2016-05-27' '2016-05-31' ... '2024-05-24' '2024-05-28'\n",
            " '2024-05-29']\n",
            "unique_dates length: 2015\n",
            "self.valid_indices head: [(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
            "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
            "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
            "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
            "       68, 69, 70, 71, 72, 73, 74]), array([75])), (array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
            "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
            "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
            "       69, 70, 71, 72, 73, 74, 75]), array([76])), (array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
            "       19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
            "       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52,\n",
            "       53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69,\n",
            "       70, 71, 72, 73, 74, 75, 76]), array([77])), (array([ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
            "       20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
            "       37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
            "       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70,\n",
            "       71, 72, 73, 74, 75, 76, 77]), array([78])), (array([ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
            "       21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
            "       38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
            "       55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
            "       72, 73, 74, 75, 76, 77, 78]), array([79])), (array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
            "       22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
            "       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
            "       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
            "       73, 74, 75, 76, 77, 78, 79]), array([80])), (array([ 6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
            "       23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39,\n",
            "       40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56,\n",
            "       57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73,\n",
            "       74, 75, 76, 77, 78, 79, 80]), array([81])), (array([ 7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n",
            "       24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n",
            "       41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57,\n",
            "       58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74,\n",
            "       75, 76, 77, 78, 79, 80, 81]), array([82])), (array([ 8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
            "       25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41,\n",
            "       42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58,\n",
            "       59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75,\n",
            "       76, 77, 78, 79, 80, 81, 82]), array([83])), (array([ 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,\n",
            "       26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42,\n",
            "       43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59,\n",
            "       60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76,\n",
            "       77, 78, 79, 80, 81, 82, 83]), array([84]))]\n",
            "self.valid_indices tail: [(array([714804, 714805, 714806, 714807, 714808, 714809, 714810, 714811,\n",
            "       714812, 714813, 714814, 714815, 714816, 714817, 714818, 714819,\n",
            "       714820, 714821, 714822, 714823, 714824, 714825, 714826, 714827,\n",
            "       714828, 714829, 714830, 714831, 714832, 714833, 714834, 714835,\n",
            "       714836, 714837, 714838, 714839, 714840, 714841, 714842, 714843,\n",
            "       714844, 714845, 714846, 714847, 714848, 714849, 714850, 714851,\n",
            "       714852, 714853, 714854, 714855, 714856, 714857, 714858, 714859,\n",
            "       714860, 714861, 714862, 714863, 714864, 714865, 714866, 714867,\n",
            "       714868, 714869, 714870, 714871, 714872, 714873, 714874, 714875,\n",
            "       714876, 714877, 714878]), array([714879])), (array([714805, 714806, 714807, 714808, 714809, 714810, 714811, 714812,\n",
            "       714813, 714814, 714815, 714816, 714817, 714818, 714819, 714820,\n",
            "       714821, 714822, 714823, 714824, 714825, 714826, 714827, 714828,\n",
            "       714829, 714830, 714831, 714832, 714833, 714834, 714835, 714836,\n",
            "       714837, 714838, 714839, 714840, 714841, 714842, 714843, 714844,\n",
            "       714845, 714846, 714847, 714848, 714849, 714850, 714851, 714852,\n",
            "       714853, 714854, 714855, 714856, 714857, 714858, 714859, 714860,\n",
            "       714861, 714862, 714863, 714864, 714865, 714866, 714867, 714868,\n",
            "       714869, 714870, 714871, 714872, 714873, 714874, 714875, 714876,\n",
            "       714877, 714878, 714879]), array([714880])), (array([714806, 714807, 714808, 714809, 714810, 714811, 714812, 714813,\n",
            "       714814, 714815, 714816, 714817, 714818, 714819, 714820, 714821,\n",
            "       714822, 714823, 714824, 714825, 714826, 714827, 714828, 714829,\n",
            "       714830, 714831, 714832, 714833, 714834, 714835, 714836, 714837,\n",
            "       714838, 714839, 714840, 714841, 714842, 714843, 714844, 714845,\n",
            "       714846, 714847, 714848, 714849, 714850, 714851, 714852, 714853,\n",
            "       714854, 714855, 714856, 714857, 714858, 714859, 714860, 714861,\n",
            "       714862, 714863, 714864, 714865, 714866, 714867, 714868, 714869,\n",
            "       714870, 714871, 714872, 714873, 714874, 714875, 714876, 714877,\n",
            "       714878, 714879, 714880]), array([714881])), (array([714807, 714808, 714809, 714810, 714811, 714812, 714813, 714814,\n",
            "       714815, 714816, 714817, 714818, 714819, 714820, 714821, 714822,\n",
            "       714823, 714824, 714825, 714826, 714827, 714828, 714829, 714830,\n",
            "       714831, 714832, 714833, 714834, 714835, 714836, 714837, 714838,\n",
            "       714839, 714840, 714841, 714842, 714843, 714844, 714845, 714846,\n",
            "       714847, 714848, 714849, 714850, 714851, 714852, 714853, 714854,\n",
            "       714855, 714856, 714857, 714858, 714859, 714860, 714861, 714862,\n",
            "       714863, 714864, 714865, 714866, 714867, 714868, 714869, 714870,\n",
            "       714871, 714872, 714873, 714874, 714875, 714876, 714877, 714878,\n",
            "       714879, 714880, 714881]), array([714882])), (array([714808, 714809, 714810, 714811, 714812, 714813, 714814, 714815,\n",
            "       714816, 714817, 714818, 714819, 714820, 714821, 714822, 714823,\n",
            "       714824, 714825, 714826, 714827, 714828, 714829, 714830, 714831,\n",
            "       714832, 714833, 714834, 714835, 714836, 714837, 714838, 714839,\n",
            "       714840, 714841, 714842, 714843, 714844, 714845, 714846, 714847,\n",
            "       714848, 714849, 714850, 714851, 714852, 714853, 714854, 714855,\n",
            "       714856, 714857, 714858, 714859, 714860, 714861, 714862, 714863,\n",
            "       714864, 714865, 714866, 714867, 714868, 714869, 714870, 714871,\n",
            "       714872, 714873, 714874, 714875, 714876, 714877, 714878, 714879,\n",
            "       714880, 714881, 714882]), array([714883])), (array([714809, 714810, 714811, 714812, 714813, 714814, 714815, 714816,\n",
            "       714817, 714818, 714819, 714820, 714821, 714822, 714823, 714824,\n",
            "       714825, 714826, 714827, 714828, 714829, 714830, 714831, 714832,\n",
            "       714833, 714834, 714835, 714836, 714837, 714838, 714839, 714840,\n",
            "       714841, 714842, 714843, 714844, 714845, 714846, 714847, 714848,\n",
            "       714849, 714850, 714851, 714852, 714853, 714854, 714855, 714856,\n",
            "       714857, 714858, 714859, 714860, 714861, 714862, 714863, 714864,\n",
            "       714865, 714866, 714867, 714868, 714869, 714870, 714871, 714872,\n",
            "       714873, 714874, 714875, 714876, 714877, 714878, 714879, 714880,\n",
            "       714881, 714882, 714883]), array([714884])), (array([714810, 714811, 714812, 714813, 714814, 714815, 714816, 714817,\n",
            "       714818, 714819, 714820, 714821, 714822, 714823, 714824, 714825,\n",
            "       714826, 714827, 714828, 714829, 714830, 714831, 714832, 714833,\n",
            "       714834, 714835, 714836, 714837, 714838, 714839, 714840, 714841,\n",
            "       714842, 714843, 714844, 714845, 714846, 714847, 714848, 714849,\n",
            "       714850, 714851, 714852, 714853, 714854, 714855, 714856, 714857,\n",
            "       714858, 714859, 714860, 714861, 714862, 714863, 714864, 714865,\n",
            "       714866, 714867, 714868, 714869, 714870, 714871, 714872, 714873,\n",
            "       714874, 714875, 714876, 714877, 714878, 714879, 714880, 714881,\n",
            "       714882, 714883, 714884]), array([714885])), (array([714811, 714812, 714813, 714814, 714815, 714816, 714817, 714818,\n",
            "       714819, 714820, 714821, 714822, 714823, 714824, 714825, 714826,\n",
            "       714827, 714828, 714829, 714830, 714831, 714832, 714833, 714834,\n",
            "       714835, 714836, 714837, 714838, 714839, 714840, 714841, 714842,\n",
            "       714843, 714844, 714845, 714846, 714847, 714848, 714849, 714850,\n",
            "       714851, 714852, 714853, 714854, 714855, 714856, 714857, 714858,\n",
            "       714859, 714860, 714861, 714862, 714863, 714864, 714865, 714866,\n",
            "       714867, 714868, 714869, 714870, 714871, 714872, 714873, 714874,\n",
            "       714875, 714876, 714877, 714878, 714879, 714880, 714881, 714882,\n",
            "       714883, 714884, 714885]), array([714886])), (array([714812, 714813, 714814, 714815, 714816, 714817, 714818, 714819,\n",
            "       714820, 714821, 714822, 714823, 714824, 714825, 714826, 714827,\n",
            "       714828, 714829, 714830, 714831, 714832, 714833, 714834, 714835,\n",
            "       714836, 714837, 714838, 714839, 714840, 714841, 714842, 714843,\n",
            "       714844, 714845, 714846, 714847, 714848, 714849, 714850, 714851,\n",
            "       714852, 714853, 714854, 714855, 714856, 714857, 714858, 714859,\n",
            "       714860, 714861, 714862, 714863, 714864, 714865, 714866, 714867,\n",
            "       714868, 714869, 714870, 714871, 714872, 714873, 714874, 714875,\n",
            "       714876, 714877, 714878, 714879, 714880, 714881, 714882, 714883,\n",
            "       714884, 714885, 714886]), array([714887])), (array([714813, 714814, 714815, 714816, 714817, 714818, 714819, 714820,\n",
            "       714821, 714822, 714823, 714824, 714825, 714826, 714827, 714828,\n",
            "       714829, 714830, 714831, 714832, 714833, 714834, 714835, 714836,\n",
            "       714837, 714838, 714839, 714840, 714841, 714842, 714843, 714844,\n",
            "       714845, 714846, 714847, 714848, 714849, 714850, 714851, 714852,\n",
            "       714853, 714854, 714855, 714856, 714857, 714858, 714859, 714860,\n",
            "       714861, 714862, 714863, 714864, 714865, 714866, 714867, 714868,\n",
            "       714869, 714870, 714871, 714872, 714873, 714874, 714875, 714876,\n",
            "       714877, 714878, 714879, 714880, 714881, 714882, 714883, 714884,\n",
            "       714885, 714886, 714887]), array([714888]))]\n",
            "self.valid_indices length: 563764\n",
            "Fold 1:\n",
            "Training dates: from 2019-01-28 to 2024-05-29\n",
            "Validation dates: from 2016-05-26 to 2019-01-25\n",
            "Training samples: 376089, Validation samples: 187675\n",
            "Epoch 1/3\n",
            "Train Loss: 0.00009403, Validation Loss: 0.00000613\n",
            "Epoch 2/3\n",
            "Train Loss: 0.00001815, Validation Loss: 0.00000513\n",
            "Epoch 3/3\n",
            "Train Loss: 0.00001756, Validation Loss: 0.00000481\n",
            "Validation Loss for fold 1: 0.00000481\n",
            "Fold 2:\n",
            "Training dates: from 2016-05-26 to 2024-05-29\n",
            "Validation dates: from 2019-01-28 to 2021-09-23\n",
            "Training samples: 376088, Validation samples: 187676\n",
            "Epoch 1/3\n",
            "Train Loss: 0.00010819, Validation Loss: 0.00001661\n",
            "Epoch 2/3\n",
            "Train Loss: 0.00001408, Validation Loss: 0.00001470\n",
            "Epoch 3/3\n",
            "Train Loss: 0.00001318, Validation Loss: 0.00001418\n",
            "Validation Loss for fold 2: 0.00001418\n",
            "Fold 3:\n",
            "Training dates: from 2016-05-26 to 2021-09-23\n",
            "Validation dates: from 2021-09-24 to 2024-05-29\n",
            "Training samples: 375351, Validation samples: 188413\n",
            "Epoch 1/3\n",
            "Train Loss: 0.00011368, Validation Loss: 0.00002177\n",
            "Epoch 2/3\n",
            "Train Loss: 0.00001013, Validation Loss: 0.00002099\n",
            "Epoch 3/3\n",
            "Train Loss: 0.00000955, Validation Loss: 0.00002063\n",
            "Validation Loss for fold 3: 0.00002063\n",
            "\n",
            "Cross-validation complete!\n",
            "Average validation loss: 0.00001321\n",
            "Standard deviation: 0.00000650\n",
            "\n",
            "Best model saved with validation loss: 0.00000481\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}